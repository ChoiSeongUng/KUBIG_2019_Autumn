{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### due date: Sat. 10/5 11:59pm\n",
    "#### 이 과제는 2학기 프로젝트 조 편성 시 반영됩니다.\n",
    "\n",
    "#### [Tutorial] #### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing data [tutorial]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean(평균)과  median(중위수)를 직접 구해봅시다. \n",
    "\n",
    "세 가지 정도의 방법을 사용해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#활용 데이터 \n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(iris.keys()) #어떤 key들을 가지고 있는지 알아봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR) \n",
    "#data description(DESCR)을 통해 어떤 데이터인지 알아봅니다 \n",
    "#각 변수 별 min, max, mean, sd(표준편차), class correlation도 확인할 수 있네요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 4.9, 4.7, 4.6, 5. , 5.4, 4.6, 5. , 4.4, 4.9, 5.4, 4.8, 4.8,\n",
       "       4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5. ,\n",
       "       5. , 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5. , 5.5, 4.9, 4.4,\n",
       "       5.1, 5. , 4.5, 4.4, 5. , 5.1, 4.8, 5.1, 4.6, 5.3, 5. , 7. , 6.4,\n",
       "       6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5. , 5.9, 6. , 6.1, 5.6,\n",
       "       6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7,\n",
       "       6. , 5.7, 5.5, 5.5, 5.8, 6. , 5.4, 6. , 6.7, 6.3, 5.6, 5.5, 5.5,\n",
       "       6.1, 5.8, 5. , 5.6, 5.7, 5.7, 6.2, 5.1, 5.7, 6.3, 5.8, 7.1, 6.3,\n",
       "       6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5,\n",
       "       7.7, 7.7, 6. , 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2,\n",
       "       7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6. , 6.9, 6.7, 6.9, 5.8,\n",
       "       6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepal_length= iris.data[:,0]     #여기서는 sepal_length 데이터를 활용하여 mean/ median을 구해봅시다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.843333333333335\n",
      "5.8\n"
     ]
    }
   ],
   "source": [
    "#1. coding from scratch\n",
    "mean1 = sum(sepal_length)/len(sepal_length)   #평균의 정의 : 총합/길이\n",
    "\n",
    "median1 = sorted(sepal_length)[len(sepal_length) // 2]   #중위수 정의 : 가운데 있는 수, //는 나눈 몫보다 크거나 같은 가장 작은 정수를 반환\n",
    "\n",
    "print(mean1)\n",
    "print(median1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.843333333333334\n",
      "5.8\n"
     ]
    }
   ],
   "source": [
    "#2. using python module\n",
    "import statistics\n",
    "\n",
    "mean2 = statistics.mean(sepal_length)\n",
    "\n",
    "median2 = statistics.median(sepal_length)\n",
    "\n",
    "print(mean2)\n",
    "print(median2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.843333333333334\n",
      "5.8\n"
     ]
    }
   ],
   "source": [
    "#3. using numpy \n",
    "import numpy as np\n",
    "mean3 = np.mean(sepal_length)\n",
    "median3 = np.median(sepal_length)\n",
    "\n",
    "print(mean3)\n",
    "print(median3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interquartile Range(IQR)를 구해봅니다 \n",
    "\n",
    "여기서도 세가지 방법을 써보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n"
     ]
    }
   ],
   "source": [
    "#1. coding from scratch\n",
    "Q1 = sorted(sepal_length)[len(sepal_length) // 4]\n",
    "Q3 = sorted(sepal_length)[3*(len(sepal_length) // 4)]\n",
    "IQR1 = Q3 - Q1\n",
    "\n",
    "print(round(IQR1,1)) #round(x,n)은 x를 소수점 n번째 자리까지 올림 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n"
     ]
    }
   ],
   "source": [
    "#2. using numpy\n",
    "Q1 = np.percentile(sepal_length, 25) #25퍼센트 백분위에 해당하는 값 Q1에 저장\n",
    "Q3 = np.percentile(sepal_length, 75)\n",
    "IQR2 = Q3 - Q1\n",
    "\n",
    "print(round(IQR2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 using pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sepal_length)\n",
    "df.describe() #사실 여기서 mean과 median(50%)도 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.3\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_info = df.describe()\n",
    "Q1 = df_info.iloc[4]\n",
    "Q3 = df_info.iloc[6]\n",
    "\n",
    "IQR3 = Q3 - Q1\n",
    "\n",
    "print(IQR3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.3\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#pandas 다른 방법\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "\n",
    "IQR_pandas = Q3 - Q1\n",
    "print(IQR_pandas)     #사실 이게 훨씬 쉬운 방법입니다. 그냥 EDA 힌트 드리려고 위의 방법도 포함합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing data [challenge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sklearn에서 datasets를 import하라\n",
    "    활용할 데이터는 datasets에서 load할 수 있는 boston데이터이다\n",
    "    boston 데이터 안에 있는 CRIM(per capita crim rate)변수의 평균과 중위수를 구하라(아무거나 골라서 해보세요, 다 해보시는 것 추천)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    boston 데이터에서 ZN(proportion of residential land zoned for lots over 25,000 sq.ft)의 IQR을 세가지 방법을 사용하여 구하라 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing value imputation [tutorial]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결측값이 있을 때는 어떡하죠...ㅠㅠ? 그 data는 못 쓰는 것인가요? 아닙니다. 지금부터 결측치 처리(missing value imputation)를 한 번 알아보도록 하겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "pregnancies    768 non-null int64\n",
      " glucose       768 non-null int64\n",
      " diastolic     768 non-null int64\n",
      " triceps       768 non-null int64\n",
      " insulin       768 non-null int64\n",
      " bmi           768 non-null float64\n",
      " dpf           768 non-null float64\n",
      " age           768 non-null int64\n",
      " diabetes      768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "#활용할 데이터 diabetes.txt 파일 \n",
    "diabetes = pd.read_csv('C:/Users/hyungjun.lim/Desktop/diabetes.txt') #read_csv()함수를 통해 불러옵니다 \n",
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Original DataFrame: (768, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Original DataFrame: {}\".format(diabetes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pregnancies   glucose   diastolic   triceps   insulin   bmi    dpf   age  \\\n",
      "0            6       148          72        35         0  33.6  0.627    50   \n",
      "1            1        85          66        29         0  26.6  0.351    31   \n",
      "2            8       183          64         0         0  23.3  0.672    32   \n",
      "3            1        89          66        23        94  28.1  0.167    21   \n",
      "4            0       137          40        35       168  43.1  2.288    33   \n",
      "\n",
      "    diabetes  \n",
      "0          1  \n",
      "1          0  \n",
      "2          1  \n",
      "3          0  \n",
      "4          1  \n"
     ]
    }
   ],
   "source": [
    "print(diabetes.head())\n",
    "\n",
    "#상식적으로 insulin수치나 triceps의 굵기가 0이라는 것은 말이 안 되죠. 결측치인겁니다.\n",
    "#결측치는 다양한 형태로 표시됩니다 0일수도 ?일수도 -1일수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "pregnancies    768 non-null int64\n",
      " glucose       768 non-null int64\n",
      " diastolic     768 non-null int64\n",
      " triceps       541 non-null float64\n",
      " insulin       394 non-null float64\n",
      " bmi           757 non-null float64\n",
      " dpf           768 non-null float64\n",
      " age           768 non-null int64\n",
      " diabetes      768 non-null int64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "# NAN(not a number)화 하기 \n",
    "diabetes.iloc[:,3].replace(0, np.NaN, inplace=True) #triceps\n",
    "diabetes.iloc[:,4].replace(0, np.NaN, inplace=True) #insulin\n",
    "diabetes.iloc[:,5].replace(0, np.NaN, inplace=True) #bmi\n",
    "diabetes.info()  #nan 대체된 곳의 total 개체 수 감소 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 9)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dropping Missing data\n",
    "# 그냥 맘편히 nan 하나라도 있는 observation(행)을 다 버려버리는 방법입니다.\n",
    "\n",
    "diabetes_drop = diabetes.dropna()\n",
    "diabetes_drop.shape #obs 수가 768에서 393으로 줄었습니다. \n",
    "\n",
    "#이처럼 버리면 맘도 편하고 데이터도 깔끔하지만 nan의 비율이 클 경우 정보의 손실이 too much한 경우가 생기겠죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame After Dropping All Rows with Missing Values: (393, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(diabetes_drop.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impution missing data \n",
    "# 쉽게 말해 결측된 부분을 guessing 해서 넣는 방법입니다. \n",
    "\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 Imputer를 설정합니다.\n",
    "\n",
    "missing_values는 아까 우리가 설정한 바와 같이 NaN입니다.\n",
    "\n",
    "strategy는 많은 방법이 있지만 같은 열의(axis=0) 평균값으로 대체하는 방법을 택했습니다. 행평균을 원할 경우(axis=1)로 설정하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0) #Imputer를 사용해 imp()함수를 설정합니다 \n",
    "\n",
    "values = diabetes.values  #diabetes DataFrame의 value만 array 형태로  뽑아냅니다. \n",
    "\n",
    "values = imp.fit_transform(values) # fit_transform 을 통해 mean imputation을 실행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "0    768 non-null float64\n",
      "1    768 non-null float64\n",
      "2    768 non-null float64\n",
      "3    768 non-null float64\n",
      "4    768 non-null float64\n",
      "5    768 non-null float64\n",
      "6    768 non-null float64\n",
      "7    768 non-null float64\n",
      "8    768 non-null float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "diabetes_impute = pd.DataFrame(values)\n",
    "diabetes_impute.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2         3           4     5      6     7    8\n",
      "0  6.0  148.0  72.0  35.00000  155.548223  33.6  0.627  50.0  1.0\n",
      "1  1.0   85.0  66.0  29.00000  155.548223  26.6  0.351  31.0  0.0\n",
      "2  8.0  183.0  64.0  29.15342  155.548223  23.3  0.672  32.0  1.0\n",
      "3  1.0   89.0  66.0  23.00000   94.000000  28.1  0.167  21.0  0.0\n",
      "4  0.0  137.0  40.0  35.00000  168.000000  43.1  2.288  33.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(diabetes_impute.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame After Mean Imputation: (768, 9)\n"
     ]
    }
   ],
   "source": [
    "## 두번째 방법 fillna 함수 사용 \n",
    "\n",
    "diabetes = pd.read_csv('C:/Users/hyungjun.lim/Desktop/diabetes.txt')\n",
    "diabetes.iloc[:,3].replace(0, np.NaN, inplace=True) #triceps 열에 대해서만 결측치 처리를 해보겠습니다.\n",
    "\n",
    "diabetes.iloc[:,3].fillna(np.mean(diabetes.iloc[:,3]))\n",
    "\n",
    "print(\"Shape of DataFrame After Mean Imputation: {}\".format(diabetes.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing value imputation [challenge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    diabetes 파일을 pd.read_csv() 함수로 불러오라\n",
    "    diabetes의 모든 결측치를 NaN으로 바꾼 후 info()함수를 사용하여 각 변수 별 감소된 total의 개수를 확인하라 \n",
    "    sklearn.preprocessing의 Imputer를 import하라\n",
    "    diabetes의 모든 결측치를 Imputer를 사용하여 평균 대체 방법으로 채워라 \n",
    "    diabetes의 모든 결측치를 fillna()함수를 사용하여 평균 대체 방법으로 채워라 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaling  [tutorial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Unscaled Features: 79.799\n",
      "Standard Deviation of Unscaled Features: 115.169\n",
      "Mean of Scaled Features: -3.0068540250264654e-17\n",
      "Standard Deviation of Scaled Features: 0.9999999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "## Scaling\n",
    "\n",
    "#sklearn에서 scale tool을 import합니다\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#이전 diabetes 데이터를 그대로 활용하도록 하겠습니다.\n",
    "diabetes = pd.read_csv('C:/Users/hyungjun.lim/Desktop/diabetes.txt')\n",
    "insulin = diabetes.iloc[:,4]\n",
    "\n",
    "# Scale the features: X_scaled\n",
    "insulin_scaled = scale(insulin)\n",
    "\n",
    "# Print the mean and standard deviation of the unscaled features\n",
    "print(\"Mean of Unscaled Features: {}\".format(round(np.mean(insulin),3))) \n",
    "print(\"Standard Deviation of Unscaled Features: {}\".format(round(np.std(insulin),3)))\n",
    "\n",
    "# Print the mean and standard deviation of the scaled features\n",
    "print(\"Mean of Scaled Features: {}\".format(np.mean(insulin_scaled))) \n",
    "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(insulin_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### centering and scaling  [challenge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        diabetes의 모든 변수를 sclaing하라"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
